{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee9PY7nutRje"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhermosillo/keyword_extraction/blob/main/API_TextRank_GFractal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D38AKJ4tRjj"
   },
   "source": [
    "# Módulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KegPrwMkeY_B",
    "outputId": "68c48c60-9d2a-463d-acc3-dccb0f240cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deplacy in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install deplacy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67kBjsfGtRjm",
    "outputId": "25c489fa-8105-478f-f8b9-7f29143b26f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting es-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.2.0/es_core_news_sm-3.2.0-py3-none-any.whl (14.0 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jorge hermosillo\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->es-core-news-sm==3.2.0) (1.1.1)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrTBJWzxepw-",
    "outputId": "9eb8affb-e8bd-4d19-91b8-99e78fc52e33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pkg_resources' from 'C:\\\\Users\\\\Jorge Hermosillo\\\\anaconda3\\\\lib\\\\site-packages\\\\pkg_resources\\\\__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pkg_resources,imp\n",
    "imp.reload(pkg_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOwjehIscqgG"
   },
   "source": [
    "### Librerías necesarias para los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fVh4oQV0brbL"
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "from math import sqrt\n",
    "import string\n",
    "import operator\n",
    "import random\n",
    "import pandas as pd\n",
    "#librerias necesarias para text rank\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "#Listado de STOPWORDS dependiendo del lenguaje\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple and orange be similar . boot and hippo be not .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "doc_l= ' '.join([token.lemma_ for token in doc])\n",
    "print(doc_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vW8QRTHbrzW"
   },
   "source": [
    "# Algoritmo TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bNqo78cRbuEO"
   },
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 100 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        keysw={}\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            keysw[key] =value\n",
    "            if i > number:\n",
    "                break\n",
    "        return keysw\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDqG-EIfg_2P"
   },
   "source": [
    "# Algoritmo Grado de Fractalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hKvAbsbrb68T"
   },
   "outputs": [],
   "source": [
    "#solamente se calcula el grado de fractalidad de las palabras que tengan mas de uno de frecuencia\n",
    "def fractalidad(palabras,vocabulario,frec,dist):\n",
    "    N=len(palabras)                                     #El número de tokens de todo el texto\n",
    "    gf={}\n",
    "    cajas_index=set()\n",
    "    voc=[]                                             #la variable voc contendra cada sintagma con frecuencia mayor que 1, por que las otras palabras tendrán 0 de grado de fractaldiad\n",
    "    for p in vocabulario:                              #Esto se puede hacer fuera del algoritmo, pero se incluye para evitar ese calculo innecesario \n",
    "        if(p not in voc):\n",
    "            if(frec[p]>1):\n",
    "                if(p not in STOP_WORDS):\n",
    "                    if(len(p)>1):\n",
    "                        voc.append(p)\n",
    "    # print(\"Text size: \",N)\n",
    "    # print(\"Vocabulary: \",len(voc))\n",
    "    for p in voc:                                  \n",
    "        rcajas=dist[p]\n",
    "        M=frec[p]                                  \n",
    "        dfw=0.0\n",
    "        nsh=0.0\n",
    "        for s in range(1,N+1):  \n",
    "            noc=0                                       \n",
    "            for e in rcajas:                       \n",
    "                cajas_index.add(ceil(int(e)/s))    \n",
    "            noc=len(cajas_index)                    \n",
    "            cajas_index.clear()    \n",
    "            ns=N/s\n",
    "            if(M<=ns):\n",
    "                nsh=M\n",
    "            else:\n",
    "                nsh=M/(1+(M-1)/(N-1)*(s-1)) \n",
    "            dfw=dfw+fabs(log(nsh/noc))\n",
    "        gf[p]=dfw\n",
    "    return gf    #regresamos un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SYUYiEhbu8Iz"
   },
   "outputs": [],
   "source": [
    "def distribucion(palabras,vocabulario):\n",
    "    N=len(palabras)\n",
    "    ncajas=[]\n",
    "    cajas={}\n",
    "    frecuencias={}\n",
    "    for p in vocabulario:\n",
    "        ncajas.clear()\n",
    "        i=0\n",
    "        M=palabras.count(p)\n",
    "        while(i<N):\n",
    "            if(p == palabras[i]):\n",
    "                ncajas.append(i+1)\n",
    "            i=i+1\n",
    "        frecuencias[p]=M\n",
    "        cajas[p]=ncajas[:]\n",
    "    return frecuencias,cajas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCxTKydwtRjw"
   },
   "source": [
    "# Lectura de archivo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PuVK_1h7tRjx"
   },
   "outputs": [],
   "source": [
    "#Lectura de archivo para generación de vocabulario\n",
    "def cargar_datos(filename):\n",
    "    f=open(filename, \"r\") #tenemos que crear un directorio llamado InputData\n",
    "    texto=f.read()\n",
    "    #Pasar a minusculas\n",
    "    texto=texto.lower()\n",
    "    #Eliminar puntuación\n",
    "    texto=texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    texto=texto.translate(str.maketrans('', '', '¿¡—“”0123456789’'))\n",
    "    palabras=texto.split()\n",
    "    textop=\"\"\n",
    "    #rearmamos el texto debido a ue existen carácteres especiales\n",
    "    for w in palabras:\n",
    "        textop=textop+w+' '\n",
    "    return textop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANaeOolE7xw4"
   },
   "source": [
    "DEFINICIÓN DEL NOMBRE DEL ARCHIVO A PROCESAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JlyILQOcvVz",
    "tags": []
   },
   "source": [
    "Lectura de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gkCfLCCDvFM_"
   },
   "outputs": [],
   "source": [
    "def lee_documento(filename='NULL',texto=''):\n",
    "    if filename != 'NULL':\n",
    "        texto=cargar_datos(filename)\n",
    "    #obtenemos el vocabulario\n",
    "    tokens=texto.split()\n",
    "    vocabulario=[]\n",
    "    for t in tokens:\n",
    "        if(t not in vocabulario):\n",
    "            vocabulario.append(t)\n",
    "    #variables de procesamiento\n",
    "    dist={}\n",
    "    frec={}\n",
    "    frec,dist=distribucion(tokens,vocabulario)\n",
    "    return frec,dist,tokens,vocabulario,texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht4genjTyIar"
   },
   "source": [
    "# Ejecución de algoritmos y generación de archivos de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cNLGwj0m6QdP"
   },
   "outputs": [],
   "source": [
    "# lectura de documento de prueba\n",
    "frec,dist,tokens,vocabulario,texto = lee_documento('data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOneVQNjtRj1"
   },
   "source": [
    "## Grado de Fractalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "aI8zOnFZcYWe",
    "outputId": "2b533e86-5398-4444-f031-5ba4e22c095f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frecuency</th>\n",
       "      <th>Degree_of_fractality</th>\n",
       "      <th>Combined_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>latin</td>\n",
       "      <td>4</td>\n",
       "      <td>167.127394</td>\n",
       "      <td>100.620717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content</td>\n",
       "      <td>3</td>\n",
       "      <td>205.903714</td>\n",
       "      <td>98.241038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humour</td>\n",
       "      <td>3</td>\n",
       "      <td>163.880039</td>\n",
       "      <td>78.190650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>injected</td>\n",
       "      <td>3</td>\n",
       "      <td>163.200270</td>\n",
       "      <td>77.866318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>191.726724</td>\n",
       "      <td>57.715495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>2</td>\n",
       "      <td>191.726724</td>\n",
       "      <td>57.715495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>line</td>\n",
       "      <td>2</td>\n",
       "      <td>179.412131</td>\n",
       "      <td>54.008433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>internet</td>\n",
       "      <td>2</td>\n",
       "      <td>175.612973</td>\n",
       "      <td>52.864773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reproduced</td>\n",
       "      <td>2</td>\n",
       "      <td>169.604962</td>\n",
       "      <td>51.056181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>typesetting</td>\n",
       "      <td>2</td>\n",
       "      <td>166.080279</td>\n",
       "      <td>49.995146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  frecuency  Degree_of_fractality  Combined_measure\n",
       "0        latin          4            167.127394        100.620717\n",
       "1      content          3            205.903714         98.241038\n",
       "2       humour          3            163.880039         78.190650\n",
       "3     injected          3            163.200270         77.866318\n",
       "4        dummy          2            191.726724         57.715495\n",
       "5         type          2            191.726724         57.715495\n",
       "6         line          2            179.412131         54.008433\n",
       "7     internet          2            175.612973         52.864773\n",
       "8   reproduced          2            169.604962         51.056181\n",
       "9  typesetting          2            166.080279         49.995146"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ejecución de algoritmo Grado de Fractalidad\n",
    "def grado_de_fractalidad(tokens,vocabulario,frec,dist,regresa_kw=False,regresa_df=True,top_n=np.inf,escribe_arch=False):\n",
    "    frac_x=fractalidad(tokens,vocabulario,frec,dist) \n",
    "    sorted_x = sorted(frac_x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    # print('Time GF: '+str(elapsed_time))\n",
    "\n",
    "    #Imprimir y guardar resultados de GF\n",
    "    if regresa_df:\n",
    "        if top_n != np.inf:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x[:top_n]]\n",
    "        else:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x]\n",
    "        #Ordenar resultados por medida combinada\n",
    "        df.sort(key=lambda x: x[3],reverse=True)\n",
    "        if regresa_kw==False:\n",
    "            df = [dato[0] for dato in df]\n",
    "            by_MC=pd.DataFrame(df, columns=['word'])\n",
    "        else:   \n",
    "            by_MC=pd.DataFrame(df, columns=['word','frecuency','Degree_of_fractality','Combined_measure'])\n",
    "        if escribe_arch:\n",
    "            by_MC.to_csv('GF.csv')\n",
    "    else:\n",
    "        if top_n != np.inf:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x[:top_n]]\n",
    "        else:\n",
    "            df=[[t[0],frec[t[0]], t[1], t[1]*log10(frec[t[0]])] for t in sorted_x]\n",
    "        #Ordenar resultados por medida combinada\n",
    "        df.sort(key=lambda x: x[3],reverse=True)\n",
    "        if regresa_kw==False:\n",
    "            by_MC = [dato[0] for dato in df]\n",
    "        else:\n",
    "            by_MC = df\n",
    "        if escribe_arch:\n",
    "            print('\\nNo se tiene implementada la escritura de archivo cuando regresa_df==False\\n')\n",
    "    return by_MC\n",
    "\n",
    "def use_gf(texto,regresa_kw=False,regresa_df=False,top_n=np.inf,escribe_arch=False):\n",
    "    tokens=texto.split()\n",
    "    vocabulario=[]\n",
    "    for t in tokens:\n",
    "        if(t not in vocabulario):\n",
    "            vocabulario.append(t)\n",
    "    #variables de procesamiento\n",
    "    dist={}\n",
    "    frec={}\n",
    "    frec,dist=distribucion(tokens,vocabulario)\n",
    "    df = grado_de_fractalidad(tokens,vocabulario,frec,dist,regresa_kw,regresa_df,top_n,escribe_arch)\n",
    "    return df\n",
    "\n",
    "df1 = use_gf(texto,regresa_kw=True,regresa_df=True,top_n=10)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu1N-IOjtRj1"
   },
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDZv8C18tRj2",
    "outputId": "edd43f69-7341-40eb-feea-cb7702df511c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ipsum',\n",
       " 'lorem',\n",
       " 'text',\n",
       " 's',\n",
       " 'versions',\n",
       " 'words',\n",
       " 'book',\n",
       " 'bc',\n",
       " 'latin',\n",
       " 'humour']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ejecución de algoritmo de TextRank\n",
    "# start_time = time()\n",
    "def use_TextRank(texto,regresa_kw=False,regresa_df=False,top_n=np.inf,escribe_arch=False):\n",
    "    tr4w = TextRank4Keyword()\n",
    "    tr4w.analyze(texto, candidate_pos = ['NOUN','PROPN'], window_size=4, lower=False)\n",
    "    kwTR=tr4w.get_keywords(100)\n",
    "\n",
    "    #Guardar resultados de TextRank\n",
    "    if regresa_df:\n",
    "        if top_n!=np.inf:\n",
    "            if regresa_kw==True:\n",
    "                salida = [[key, kwTR[key]] for key in kwTR.keys()][:top_n]\n",
    "                dftr=pd.DataFrame(salida, columns=['word', 'Index'])\n",
    "            else:\n",
    "                salida = list(kwTR.keys())[:top_n]\n",
    "                dftr=pd.DataFrame(salida, columns=['word'])\n",
    "        else:\n",
    "            if regresa_kw==True:\n",
    "                salida = [[key, kwTR[key]] for key in kwTR.keys()]\n",
    "                dftr=pd.DataFrame(salida, columns=['word', 'Index'])\n",
    "            else:\n",
    "                salida = list(kwTR.keys())\n",
    "                dftr=pd.DataFrame(salida, columns=['word'])\n",
    "    else:\n",
    "        if top_n!=np.inf:\n",
    "            if regresa_kw==True:\n",
    "                dftr = [[key, kwTR[key]] for key in kwTR.keys()][:top_n]\n",
    "            else:\n",
    "                dftr = list(kwTR.keys())[:top_n]\n",
    "        else:\n",
    "            if regresa_kw==True:\n",
    "                dftr = [[key, kwTR[key]] for key in kwTR.keys()]\n",
    "            else:\n",
    "                dftr = list(kwTR.keys())\n",
    "        # elapsed_time = time() - start_time\n",
    "        # print('Time TextRank: '+str(elapsed_time))\n",
    "        if escribe_arch:\n",
    "            dftr.to_csv('TextRank.csv')\n",
    "    return dftr\n",
    "\n",
    "dftr = use_TextRank(texto,top_n=10)\n",
    "dftr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-fgtO1StRj3"
   },
   "source": [
    "# 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pM0Af8l7tRj4"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(r,remove_STW=True,lemmatize=False):\n",
    "    def check_STW(palabras):\n",
    "        texto = [w for w in palabras \\\n",
    "                 if not w in STOP_WORDS and len(w)>2]\n",
    "        return texto\n",
    "    r1=r.cadena.str.translate(\\\n",
    "          str.maketrans('','',string.digits))\\\n",
    "          .str.translate(\\\n",
    "          str.maketrans('','',string.punctuation))\\\n",
    "          .str.replace('«','', regex=True)\\\n",
    "          .str.replace('»','', regex=True).str.replace('(','', regex=True)\\\n",
    "          .str.replace('\\n',' ', regex=True)\\\n",
    "          .str.replace(')','', regex=True).str.strip().str.lower()\n",
    "    if remove_STW:\n",
    "        r1=r1.to_frame().applymap(lambda x : x.split()).applymap(check_STW).applymap(lambda x: ' '.join(x))\n",
    "    if lemmatize:\n",
    "        if isinstance(r1, pd.DataFrame):\n",
    "            r1=r1.applymap(lambda x : nlp(x)).applymap(lambda x: ' '.join([token.lemma_ for token in x]))        \n",
    "        else:\n",
    "            r1=r1.to_frame().applymap(lambda x : nlp(x)).applymap(lambda x: ' '.join([token.lemma_ for token in x]))        \n",
    "    r.cadena = r1\n",
    "    r=r.rename(columns={'cadena':'docs'})\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dX_xYB1CQPwm",
    "outputId": "111123c9-fa6e-4d39-a822-21c4a932563d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball']\n",
      "['rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "from pprint import pprint\n",
    "# pprint(list(newsgroups_train.target_names))\n",
    "\n",
    "cats=list(newsgroups_train.target_names)\n",
    "n = len(cats)//2\n",
    "cats1=cats[:n]\n",
    "cats2=cats[n:]\n",
    "# print(cats)\n",
    "print(cats1)\n",
    "print(cats2)\n",
    "categories = [cats1,cats2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Rq4nShdDDzqZ"
   },
   "outputs": [],
   "source": [
    "def process_20NG(categories,n=20,th=5,remove_STW=True,lemmatize=False):\n",
    "    data=[]\n",
    "    for i,cats in enumerate(categories):\n",
    "        newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                        remove=('headers', 'footers', 'quotes'),\n",
    "                                        categories=cats)\n",
    "        df = pd.DataFrame(newsgroups_train.data, columns=['cadena'])\n",
    "        df['target'] = pd.Series(newsgroups_train.target+len(cats)*i)\n",
    "        df = preprocess_text(df,remove_STW=remove_STW,lemmatize=lemmatize)\n",
    "        df['TR'] = df.docs.apply(lambda x : use_TextRank(x,top_n=n))\n",
    "        df['GF'] = df.docs.apply(lambda x : use_gf(x,top_n=n))\n",
    "        df=df.drop(df[df.TR.apply(lambda x: len(x) < th)].index).reset_index(drop=True)\n",
    "        df=df.drop(df[df.GF.apply(lambda x: len(x) < th)].index).reset_index(drop=True)\n",
    "        data.append(df)\n",
    "    lista=range(len(categories))\n",
    "    newsgroups_data = data[0]\n",
    "    if len(lista)>1:\n",
    "        for j in lista[1:]:\n",
    "            newsgroups_data = newsgroups_data.append(data[j],ignore_index=True)\n",
    "    newsgroups_data = newsgroups_data.sample(frac=1).reset_index(drop=True)\n",
    "    # newsgroups_data.to_csv('newsgroups_train_data.csv')\n",
    "    return newsgroups_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "NLlshR5zIiba",
    "outputId": "7e1daecf-ce6f-4f26-bb97-21d6564f5cad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>target</th>\n",
       "      <th>TR</th>\n",
       "      <th>GF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>koc respond article aprurartusdpaorg dbdurartu...</td>\n",
       "      <td>17</td>\n",
       "      <td>[trebizon, time, soldier, fact, massacre, pers...</td>\n",
       "      <td>[persian, time, fact, trebizon, armenians, sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clone count poor try clone go regular modifica...</td>\n",
       "      <td>2</td>\n",
       "      <td>[adobe, level, book, implementation, file, ven...</td>\n",
       "      <td>[poor, clone, red, book, adobe, use, file, fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nhl playoff result conference semifinal good s...</td>\n",
       "      <td>10</td>\n",
       "      <td>[lead, det, period, van, tor, wing, canuck, ad...</td>\n",
       "      <td>[shot, save, det, van, period, canuck, red, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsfrom staffordvaxwinonamsusedu john stafford ...</td>\n",
       "      <td>8</td>\n",
       "      <td>[ride, stafford, dod, live, lucas, slmr, discl...</td>\n",
       "      <td>[bad, ride, dod, stafford, live, like, temporary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact matter poverty imperfectly relate social ...</td>\n",
       "      <td>18</td>\n",
       "      <td>[revolution, welfare, poverty, constitution, s...</td>\n",
       "      <td>[welfare, general, revolution, include, social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>original poster misquote reference tim author ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[god, jesus, bible, spirit, author, revelation...</td>\n",
       "      <td>[god, spirit, saul, jesus, bible, claim, revel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>need help zxa supertrapp slipon carb rejette m...</td>\n",
       "      <td>8</td>\n",
       "      <td>[carb, tune, match, carbs, rejette, gear, bike...</td>\n",
       "      <td>[carb, tune, flow, cbr, fix, match, lean, new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>source code macintosh pgp available anonymous ...</td>\n",
       "      <td>11</td>\n",
       "      <td>[ftp, source, site, documentation, cipher, sig...</td>\n",
       "      <td>[public, source, key, executable, site, cipher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>yeah right let guy write piece title imply cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>[propaganda, title, piece, respond, discuss, e...</td>\n",
       "      <td>[respond, piece, propaganda, real, title, want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>expect run time factor well know algorithm fin...</td>\n",
       "      <td>11</td>\n",
       "      <td>[discussion, assumption, position, time, rubic...</td>\n",
       "      <td>[know, post, want, assumption, rubick, cube, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4934 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   docs  target  \\\n",
       "0     koc respond article aprurartusdpaorg dbdurartu...      17   \n",
       "1     clone count poor try clone go regular modifica...       2   \n",
       "2     nhl playoff result conference semifinal good s...      10   \n",
       "3     jsfrom staffordvaxwinonamsusedu john stafford ...       8   \n",
       "4     fact matter poverty imperfectly relate social ...      18   \n",
       "...                                                 ...     ...   \n",
       "4929  original poster misquote reference tim author ...      15   \n",
       "4930  need help zxa supertrapp slipon carb rejette m...       8   \n",
       "4931  source code macintosh pgp available anonymous ...      11   \n",
       "4932  yeah right let guy write piece title imply cas...       0   \n",
       "4933  expect run time factor well know algorithm fin...      11   \n",
       "\n",
       "                                                     TR  \\\n",
       "0     [trebizon, time, soldier, fact, massacre, pers...   \n",
       "1     [adobe, level, book, implementation, file, ven...   \n",
       "2     [lead, det, period, van, tor, wing, canuck, ad...   \n",
       "3     [ride, stafford, dod, live, lucas, slmr, discl...   \n",
       "4     [revolution, welfare, poverty, constitution, s...   \n",
       "...                                                 ...   \n",
       "4929  [god, jesus, bible, spirit, author, revelation...   \n",
       "4930  [carb, tune, match, carbs, rejette, gear, bike...   \n",
       "4931  [ftp, source, site, documentation, cipher, sig...   \n",
       "4932  [propaganda, title, piece, respond, discuss, e...   \n",
       "4933  [discussion, assumption, position, time, rubic...   \n",
       "\n",
       "                                                     GF  \n",
       "0     [persian, time, fact, trebizon, armenians, sol...  \n",
       "1     [poor, clone, red, book, adobe, use, file, fol...  \n",
       "2     [shot, save, det, van, period, canuck, red, wi...  \n",
       "3     [bad, ride, dod, stafford, live, like, temporary]  \n",
       "4     [welfare, general, revolution, include, social...  \n",
       "...                                                 ...  \n",
       "4929  [god, spirit, saul, jesus, bible, claim, revel...  \n",
       "4930  [carb, tune, flow, cbr, fix, match, lean, new,...  \n",
       "4931  [public, source, key, executable, site, cipher...  \n",
       "4932    [respond, piece, propaganda, real, title, want]  \n",
       "4933  [know, post, want, assumption, rubick, cube, d...  \n",
       "\n",
       "[4934 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train_data = process_20NG(categories,remove_STW=True,lemmatize=True)\n",
    "newsgroups_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_M8Sozg2_xP",
    "outputId": "129e270c-dd37-45fc-801e-3c6a5c20906e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[list(['persian', 'time', 'fact', 'trebizon', 'armenians', 'soldier', 'koc', 'armenian', 'leave', 'shirak', 'van', 'number', 'alive', 'army', 'massacre', 'story', 'destroy', 'slaughter', 'strike'])\n",
      " list(['poor', 'clone', 'red', 'book', 'adobe', 'use', 'file', 'follow', 'level', 'vendor', 'implementation', 'big', 'test', 'source', 'bug', 'pscriptdrv', 'design', 'printer', 'number', 'issue'])\n",
      " list(['shot', 'save', 'det', 'van', 'period', 'canuck', 'red', 'win', 'maple', 'courtnall', 'bure', 'coffey', 'linden', 'drake', 'att', 'powerplay', 'goal', 'winnipeg', 'vancouver', 'jet'])\n",
      " list(['bad', 'ride', 'dod', 'stafford', 'live', 'like', 'temporary'])\n",
      " list(['welfare', 'general', 'revolution', 'include', 'social', 'poverty', 'relate', 'typical', 'guess', 'section', 'constitution', 'article', 'inequality', 'instability', 'fact'])]\n",
      "[list(['trebizon', 'time', 'soldier', 'fact', 'massacre', 'persian', 'story', 'armenians', 'slaughter', 'koc', 'number', 'army', 'van', 'shirak', 'aprurartusdpaorg', 'problem', 'father', 'follow', 'prize', 'war'])\n",
      " list(['adobe', 'level', 'book', 'implementation', 'file', 'vendor', 'design', 'stuff', 'number', 'pscriptdrv', 'bug', 'printer', 'clone', 'test', 'follow', 'source', 'issue', 'patience', 'genicom', 'face'])\n",
      " list(['lead', 'det', 'period', 'van', 'tor', 'wing', 'canuck', 'adams', 'powerplay', 'win', 'goal', 'racine', 'maple', 'cullen', 'winnipeg', 'jet', 'courtnall', 'vancouver', 'bure', 'detroit'])\n",
      " list(['ride', 'stafford', 'dod', 'live', 'lucas', 'slmr', 'disclaimer', 'jsin', 'article', 'winona', 'build', 'address', 'vinyl', 'charlie', 'university', 'geeky', 'reply', 'mid', 'area', 'razzing'])\n",
      " list(['revolution', 'welfare', 'poverty', 'constitution', 'section', 'inequality', 'article', 'instability', 'fact', 'power', 'congress', 'china', 'taxesto', 'defense', 'economy', 'revolutions', 'defence', 'debt', 'portion', 'russia'])]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>target</th>\n",
       "      <th>TR</th>\n",
       "      <th>GF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>koc respond article aprurartusdpaorg dbdurartu...</td>\n",
       "      <td>17</td>\n",
       "      <td>[trebizon, time, soldier, fact, massacre, pers...</td>\n",
       "      <td>[persian, time, fact, trebizon, armenians, sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clone count poor try clone go regular modifica...</td>\n",
       "      <td>2</td>\n",
       "      <td>[adobe, level, book, implementation, file, ven...</td>\n",
       "      <td>[poor, clone, red, book, adobe, use, file, fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nhl playoff result conference semifinal good s...</td>\n",
       "      <td>10</td>\n",
       "      <td>[lead, det, period, van, tor, wing, canuck, ad...</td>\n",
       "      <td>[shot, save, det, van, period, canuck, red, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsfrom staffordvaxwinonamsusedu john stafford ...</td>\n",
       "      <td>8</td>\n",
       "      <td>[ride, stafford, dod, live, lucas, slmr, discl...</td>\n",
       "      <td>[bad, ride, dod, stafford, live, like, temporary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact matter poverty imperfectly relate social ...</td>\n",
       "      <td>18</td>\n",
       "      <td>[revolution, welfare, poverty, constitution, s...</td>\n",
       "      <td>[welfare, general, revolution, include, social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>original poster misquote reference tim author ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[god, jesus, bible, spirit, author, revelation...</td>\n",
       "      <td>[god, spirit, saul, jesus, bible, claim, revel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>need help zxa supertrapp slipon carb rejette m...</td>\n",
       "      <td>8</td>\n",
       "      <td>[carb, tune, match, carbs, rejette, gear, bike...</td>\n",
       "      <td>[carb, tune, flow, cbr, fix, match, lean, new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>source code macintosh pgp available anonymous ...</td>\n",
       "      <td>11</td>\n",
       "      <td>[ftp, source, site, documentation, cipher, sig...</td>\n",
       "      <td>[public, source, key, executable, site, cipher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>yeah right let guy write piece title imply cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>[propaganda, title, piece, respond, discuss, e...</td>\n",
       "      <td>[respond, piece, propaganda, real, title, want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>expect run time factor well know algorithm fin...</td>\n",
       "      <td>11</td>\n",
       "      <td>[discussion, assumption, position, time, rubic...</td>\n",
       "      <td>[know, post, want, assumption, rubick, cube, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4934 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   docs  target  \\\n",
       "0     koc respond article aprurartusdpaorg dbdurartu...      17   \n",
       "1     clone count poor try clone go regular modifica...       2   \n",
       "2     nhl playoff result conference semifinal good s...      10   \n",
       "3     jsfrom staffordvaxwinonamsusedu john stafford ...       8   \n",
       "4     fact matter poverty imperfectly relate social ...      18   \n",
       "...                                                 ...     ...   \n",
       "4929  original poster misquote reference tim author ...      15   \n",
       "4930  need help zxa supertrapp slipon carb rejette m...       8   \n",
       "4931  source code macintosh pgp available anonymous ...      11   \n",
       "4932  yeah right let guy write piece title imply cas...       0   \n",
       "4933  expect run time factor well know algorithm fin...      11   \n",
       "\n",
       "                                                     TR  \\\n",
       "0     [trebizon, time, soldier, fact, massacre, pers...   \n",
       "1     [adobe, level, book, implementation, file, ven...   \n",
       "2     [lead, det, period, van, tor, wing, canuck, ad...   \n",
       "3     [ride, stafford, dod, live, lucas, slmr, discl...   \n",
       "4     [revolution, welfare, poverty, constitution, s...   \n",
       "...                                                 ...   \n",
       "4929  [god, jesus, bible, spirit, author, revelation...   \n",
       "4930  [carb, tune, match, carbs, rejette, gear, bike...   \n",
       "4931  [ftp, source, site, documentation, cipher, sig...   \n",
       "4932  [propaganda, title, piece, respond, discuss, e...   \n",
       "4933  [discussion, assumption, position, time, rubic...   \n",
       "\n",
       "                                                     GF  \n",
       "0     [persian, time, fact, trebizon, armenians, sol...  \n",
       "1     [poor, clone, red, book, adobe, use, file, fol...  \n",
       "2     [shot, save, det, van, period, canuck, red, wi...  \n",
       "3     [bad, ride, dod, stafford, live, like, temporary]  \n",
       "4     [welfare, general, revolution, include, social...  \n",
       "...                                                 ...  \n",
       "4929  [god, spirit, saul, jesus, bible, claim, revel...  \n",
       "4930  [carb, tune, flow, cbr, fix, match, lean, new,...  \n",
       "4931  [public, source, key, executable, site, cipher...  \n",
       "4932    [respond, piece, propaganda, real, title, want]  \n",
       "4933  [know, post, want, assumption, rubick, cube, d...  \n",
       "\n",
       "[4934 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(newsgroups_train_data.target))\n",
    "print(newsgroups_train_data.GF.values[:5])\n",
    "print(newsgroups_train_data.TR.values[:5])\n",
    "df = newsgroups_train_data.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TblnRjmiFMd1"
   },
   "outputs": [],
   "source": [
    "newsgroups_train_data.to_csv('newsgroups_train_data_sinSW_lemma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "W2V    [0.63286227, 1.7717185, -0.47656956, 0.7558266...\n",
      "Name: sound, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W2V</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Palabra</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>koc</th>\n",
       "      <td>[0.007138354, 0.002010935, -0.03214333, 0.0123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respond</th>\n",
       "      <td>[-0.016314097, 0.905852, -0.80287933, 0.102970...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <td>[-0.21221071, 0.28731912, -1.1636721, -0.44954...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aprurartusdpaorg</th>\n",
       "      <td>[0.0031581398, 0.0013548204, -0.018494118, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbdurartu</th>\n",
       "      <td>[-0.008125715, 0.005395709, -0.009346178, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                W2V\n",
       "Palabra                                                            \n",
       "koc               [0.007138354, 0.002010935, -0.03214333, 0.0123...\n",
       "respond           [-0.016314097, 0.905852, -0.80287933, 0.102970...\n",
       "article           [-0.21221071, 0.28731912, -1.1636721, -0.44954...\n",
       "aprurartusdpaorg  [0.0031581398, 0.0013548204, -0.018494118, -0....\n",
       "dbdurartu         [-0.008125715, 0.005395709, -0.009346178, 0.00..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import gensim.downloader as api\n",
    "\n",
    "def vocdf(df,vec_size=100):\n",
    "    # obtain vocabulary word types \n",
    "    types=df['docs'].str.split(' ', expand=True).stack().unique()\n",
    "    # Data Frame of vocabulary and word embeddings\n",
    "    typesDF=pd.Series(types).to_frame()\n",
    "    typesDF.rename(index=int,columns={0:'Palabra'},inplace=True)\n",
    "\n",
    "    #Add Emebddings placeholders\n",
    "    #Se necesita convertir el DF a diccionario\n",
    "    #luego se agregan vectores de dimension N,\n",
    "    #como registros nuevos del diccionario\n",
    "    #para reconvertirlo en un DF de vuelta\n",
    "    dico=typesDF.to_dict('records',into=OrderedDict)\n",
    "    #Add real-valued embedding vectors\n",
    "    for reg in dico:\n",
    "        reg['W2V']=np.zeros(vec_size)\n",
    "    typesDF=pd.DataFrame.from_dict(dico)\n",
    "    typesDF.set_index('Palabra',inplace=True)\n",
    "    return typesDF\n",
    "\n",
    "def compute_embeddings(df,typesdf,vec_dim=100):\n",
    "    #Nos quedamos con la columna que nos importa, la que contiene las cadenas\n",
    "    docs = df[\"docs\"].values.tolist()\n",
    "    docs = [s.split() for s in docs]\n",
    "    model = w2v(docs, min_count=1, vector_size=vec_dim)\n",
    "    words = typesdf.index.values.tolist()\n",
    "    for w in words:\n",
    "        typesdf.at[w,'W2V'] = model.wv[w]\n",
    "    return\n",
    "\n",
    "typesDF = vocdf(df)\n",
    "print(typesDF.iloc[0].W2V.shape)\n",
    "\n",
    "compute_embeddings(df,typesDF)\n",
    "print(typesDF.loc['sound'])\n",
    "typesDF.head()\n",
    "# model.wv['sound']\n",
    "# model.wv.most_similar('good', topn=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construye los arreglos de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>target</th>\n",
       "      <th>TR</th>\n",
       "      <th>GF</th>\n",
       "      <th>VTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>koc respond article aprurartusdpaorg dbdurartu...</td>\n",
       "      <td>17</td>\n",
       "      <td>[trebizon, time, soldier, fact, massacre, pers...</td>\n",
       "      <td>[persian, time, fact, trebizon, armenians, sol...</td>\n",
       "      <td>[[0.0017461785, 0.051693786, -0.016429123, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clone count poor try clone go regular modifica...</td>\n",
       "      <td>2</td>\n",
       "      <td>[adobe, level, book, implementation, file, ven...</td>\n",
       "      <td>[poor, clone, red, book, adobe, use, file, fol...</td>\n",
       "      <td>[[0.16018964, 0.12162198, -0.10424078, -0.0826...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nhl playoff result conference semifinal good s...</td>\n",
       "      <td>10</td>\n",
       "      <td>[lead, det, period, van, tor, wing, canuck, ad...</td>\n",
       "      <td>[shot, save, det, van, period, canuck, red, wi...</td>\n",
       "      <td>[[0.587381, 1.2823002, -0.26724386, 0.3588745,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsfrom staffordvaxwinonamsusedu john stafford ...</td>\n",
       "      <td>8</td>\n",
       "      <td>[ride, stafford, dod, live, lucas, slmr, discl...</td>\n",
       "      <td>[bad, ride, dod, stafford, live, like, temporary]</td>\n",
       "      <td>[[0.66758555, 1.4934801, -0.28840137, 0.179831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact matter poverty imperfectly relate social ...</td>\n",
       "      <td>18</td>\n",
       "      <td>[revolution, welfare, poverty, constitution, s...</td>\n",
       "      <td>[welfare, general, revolution, include, social...</td>\n",
       "      <td>[[0.26836053, 0.33547762, -0.1683432, -0.01250...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  target  \\\n",
       "0  koc respond article aprurartusdpaorg dbdurartu...      17   \n",
       "1  clone count poor try clone go regular modifica...       2   \n",
       "2  nhl playoff result conference semifinal good s...      10   \n",
       "3  jsfrom staffordvaxwinonamsusedu john stafford ...       8   \n",
       "4  fact matter poverty imperfectly relate social ...      18   \n",
       "\n",
       "                                                  TR  \\\n",
       "0  [trebizon, time, soldier, fact, massacre, pers...   \n",
       "1  [adobe, level, book, implementation, file, ven...   \n",
       "2  [lead, det, period, van, tor, wing, canuck, ad...   \n",
       "3  [ride, stafford, dod, live, lucas, slmr, discl...   \n",
       "4  [revolution, welfare, poverty, constitution, s...   \n",
       "\n",
       "                                                  GF  \\\n",
       "0  [persian, time, fact, trebizon, armenians, sol...   \n",
       "1  [poor, clone, red, book, adobe, use, file, fol...   \n",
       "2  [shot, save, det, van, period, canuck, red, wi...   \n",
       "3  [bad, ride, dod, stafford, live, like, temporary]   \n",
       "4  [welfare, general, revolution, include, social...   \n",
       "\n",
       "                                                 VTR  \n",
       "0  [[0.0017461785, 0.051693786, -0.016429123, 0.0...  \n",
       "1  [[0.16018964, 0.12162198, -0.10424078, -0.0826...  \n",
       "2  [[0.587381, 1.2823002, -0.26724386, 0.3588745,...  \n",
       "3  [[0.66758555, 1.4934801, -0.28840137, 0.179831...  \n",
       "4  [[0.26836053, 0.33547762, -0.1683432, -0.01250...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['VTR'] = df.TR.apply(lambda x: np.array([vec for word in x for vec in typesDF.loc[word]]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>target</th>\n",
       "      <th>TR</th>\n",
       "      <th>GF</th>\n",
       "      <th>VTR</th>\n",
       "      <th>VGF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>koc respond article aprurartusdpaorg dbdurartu...</td>\n",
       "      <td>17</td>\n",
       "      <td>[trebizon, time, soldier, fact, massacre, pers...</td>\n",
       "      <td>[persian, time, fact, trebizon, armenians, sol...</td>\n",
       "      <td>[[0.0017461785, 0.051693786, -0.016429123, 0.0...</td>\n",
       "      <td>[[0.20330037, 0.26017433, -0.11627525, -0.0083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clone count poor try clone go regular modifica...</td>\n",
       "      <td>2</td>\n",
       "      <td>[adobe, level, book, implementation, file, ven...</td>\n",
       "      <td>[poor, clone, red, book, adobe, use, file, fol...</td>\n",
       "      <td>[[0.16018964, 0.12162198, -0.10424078, -0.0826...</td>\n",
       "      <td>[[0.5351642, 1.1958764, -0.3077469, 0.2469886,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nhl playoff result conference semifinal good s...</td>\n",
       "      <td>10</td>\n",
       "      <td>[lead, det, period, van, tor, wing, canuck, ad...</td>\n",
       "      <td>[shot, save, det, van, period, canuck, red, wi...</td>\n",
       "      <td>[[0.587381, 1.2823002, -0.26724386, 0.3588745,...</td>\n",
       "      <td>[[0.7719148, 1.3434646, -0.37406495, 0.0462163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsfrom staffordvaxwinonamsusedu john stafford ...</td>\n",
       "      <td>8</td>\n",
       "      <td>[ride, stafford, dod, live, lucas, slmr, discl...</td>\n",
       "      <td>[bad, ride, dod, stafford, live, like, temporary]</td>\n",
       "      <td>[[0.66758555, 1.4934801, -0.28840137, 0.179831...</td>\n",
       "      <td>[[0.6132214, 2.5334766, -0.33142954, 0.8282831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact matter poverty imperfectly relate social ...</td>\n",
       "      <td>18</td>\n",
       "      <td>[revolution, welfare, poverty, constitution, s...</td>\n",
       "      <td>[welfare, general, revolution, include, social...</td>\n",
       "      <td>[[0.26836053, 0.33547762, -0.1683432, -0.01250...</td>\n",
       "      <td>[[0.18528216, 0.31097674, -0.17614472, 0.04937...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docs  target  \\\n",
       "0  koc respond article aprurartusdpaorg dbdurartu...      17   \n",
       "1  clone count poor try clone go regular modifica...       2   \n",
       "2  nhl playoff result conference semifinal good s...      10   \n",
       "3  jsfrom staffordvaxwinonamsusedu john stafford ...       8   \n",
       "4  fact matter poverty imperfectly relate social ...      18   \n",
       "\n",
       "                                                  TR  \\\n",
       "0  [trebizon, time, soldier, fact, massacre, pers...   \n",
       "1  [adobe, level, book, implementation, file, ven...   \n",
       "2  [lead, det, period, van, tor, wing, canuck, ad...   \n",
       "3  [ride, stafford, dod, live, lucas, slmr, discl...   \n",
       "4  [revolution, welfare, poverty, constitution, s...   \n",
       "\n",
       "                                                  GF  \\\n",
       "0  [persian, time, fact, trebizon, armenians, sol...   \n",
       "1  [poor, clone, red, book, adobe, use, file, fol...   \n",
       "2  [shot, save, det, van, period, canuck, red, wi...   \n",
       "3  [bad, ride, dod, stafford, live, like, temporary]   \n",
       "4  [welfare, general, revolution, include, social...   \n",
       "\n",
       "                                                 VTR  \\\n",
       "0  [[0.0017461785, 0.051693786, -0.016429123, 0.0...   \n",
       "1  [[0.16018964, 0.12162198, -0.10424078, -0.0826...   \n",
       "2  [[0.587381, 1.2823002, -0.26724386, 0.3588745,...   \n",
       "3  [[0.66758555, 1.4934801, -0.28840137, 0.179831...   \n",
       "4  [[0.26836053, 0.33547762, -0.1683432, -0.01250...   \n",
       "\n",
       "                                                 VGF  \n",
       "0  [[0.20330037, 0.26017433, -0.11627525, -0.0083...  \n",
       "1  [[0.5351642, 1.1958764, -0.3077469, 0.2469886,...  \n",
       "2  [[0.7719148, 1.3434646, -0.37406495, 0.0462163...  \n",
       "3  [[0.6132214, 2.5334766, -0.33142954, 0.8282831...  \n",
       "4  [[0.18528216, 0.31097674, -0.17614472, 0.04937...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['VGF'] = df.GF.apply(lambda x: np.array([vec for word in x for vec in typesDF.loc[word]]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding de arreglos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(X,n,vec_dim):\n",
    "    pad_vec = np.zeros(vec_dim)\n",
    "    for i,x in enumerate(X):\n",
    "        if x[0].shape[0] != n:\n",
    "            fn = n - x[0].shape[0]\n",
    "            padx = x[0].tolist()\n",
    "            for j in range(fn):\n",
    "                padx.append(pad_vec)\n",
    "            X[i][0]=np.array(padx)\n",
    "        else:\n",
    "            X[i][0]=x[0]\n",
    "    return\n",
    "\n",
    "def repadding(X,n,vec_dim):\n",
    "    pad_vec = np.zeros(vec_dim)\n",
    "    newX = []\n",
    "    for x in X:\n",
    "        if x.shape[0] != n:\n",
    "            fn = n - x.shape[0]\n",
    "            padx = x.tolist()\n",
    "            for j in range(fn):\n",
    "                padx.append(pad_vec)\n",
    "            newX.append(padx)\n",
    "        else:\n",
    "            newX.append(x)\n",
    "    return np.array(newX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0017461785, 0.051693786, -0.016429123, 0.0...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.16018964, 0.12162198, -0.10424078, -0.0826...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.587381, 1.2823002, -0.26724386, 0.3588745,...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.66758555, 1.4934801, -0.28840137, 0.179831...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.26836053, 0.33547762, -0.1683432, -0.01250...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>[[0.37792504, 2.9643931, -0.88236153, 1.737902...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>[[0.20676619, 0.26823014, -0.16241468, 0.01207...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>[[0.77825636, 0.29552558, -1.9968874, -0.85939...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>[[0.32636356, 0.515005, -0.31521046, 0.0769180...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>[[0.081341796, 1.2409571, -0.94489336, 0.28359...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4934 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    vec target\n",
       "0     [[0.0017461785, 0.051693786, -0.016429123, 0.0...     17\n",
       "1     [[0.16018964, 0.12162198, -0.10424078, -0.0826...      2\n",
       "2     [[0.587381, 1.2823002, -0.26724386, 0.3588745,...     10\n",
       "3     [[0.66758555, 1.4934801, -0.28840137, 0.179831...      8\n",
       "4     [[0.26836053, 0.33547762, -0.1683432, -0.01250...     18\n",
       "...                                                 ...    ...\n",
       "4929  [[0.37792504, 2.9643931, -0.88236153, 1.737902...     15\n",
       "4930  [[0.20676619, 0.26823014, -0.16241468, 0.01207...      8\n",
       "4931  [[0.77825636, 0.29552558, -1.9968874, -0.85939...     11\n",
       "4932  [[0.32636356, 0.515005, -0.31521046, 0.0769180...      0\n",
       "4933  [[0.081341796, 1.2409571, -0.94489336, 0.28359...     11\n",
       "\n",
       "[4934 rows x 2 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_VTR = df.VTR.values\n",
    "X_VTR = X_VTR.reshape(X_VTR.shape[0],1)\n",
    "y_VTR = df.target.values\n",
    "y_VTR = y_VTR.reshape(y_VTR.shape[0],1)\n",
    "VTR_train = np.hstack((X_VTR,y_VTR))\n",
    "padding(VTR_train,20,100)\n",
    "VTR_train = pd.DataFrame(VTR_train,columns=['vec','target'])\n",
    "VTR_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0017461785, 0.051693786, -0.016429123, 0.0...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.16018964, 0.12162198, -0.10424078, -0.0826...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.587381, 1.2823002, -0.26724386, 0.3588745,...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.66758555, 1.4934801, -0.28840137, 0.179831...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.26836053, 0.33547762, -0.1683432, -0.01250...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>[[0.37792504, 2.9643931, -0.88236153, 1.737902...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>[[0.20676619, 0.26823014, -0.16241468, 0.01207...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>[[0.77825636, 0.29552558, -1.9968874, -0.85939...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>[[0.32636356, 0.515005, -0.31521046, 0.0769180...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>[[0.081341796, 1.2409571, -0.94489336, 0.28359...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4934 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    vec target\n",
       "0     [[0.0017461785, 0.051693786, -0.016429123, 0.0...     17\n",
       "1     [[0.16018964, 0.12162198, -0.10424078, -0.0826...      2\n",
       "2     [[0.587381, 1.2823002, -0.26724386, 0.3588745,...     10\n",
       "3     [[0.66758555, 1.4934801, -0.28840137, 0.179831...      8\n",
       "4     [[0.26836053, 0.33547762, -0.1683432, -0.01250...     18\n",
       "...                                                 ...    ...\n",
       "4929  [[0.37792504, 2.9643931, -0.88236153, 1.737902...     15\n",
       "4930  [[0.20676619, 0.26823014, -0.16241468, 0.01207...      8\n",
       "4931  [[0.77825636, 0.29552558, -1.9968874, -0.85939...     11\n",
       "4932  [[0.32636356, 0.515005, -0.31521046, 0.0769180...      0\n",
       "4933  [[0.081341796, 1.2409571, -0.94489336, 0.28359...     11\n",
       "\n",
       "[4934 rows x 2 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_VGF = df.VTR.values\n",
    "X_VGF = X_VGF.reshape(X_VGF.shape[0],1)\n",
    "y_VGF = df.target.values\n",
    "y_VGF = y_VGF.reshape(y_VGF.shape[0],1)\n",
    "VGF_train = np.hstack((X_VGF,y_VGF))\n",
    "padding(VGF_train,20,100)\n",
    "VGF_train = pd.DataFrame(VGF_train,columns=['vec','target'])\n",
    "VGF_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup de datos para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "VTR_val = VTR_train.sample(frac=0.1)\n",
    "VTR_train = VTR_train.drop(labels=VTR_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4441, 20, 100)\n",
      "(4441,)\n",
      "(493, 20, 100)\n",
      "(493,)\n"
     ]
    }
   ],
   "source": [
    "X_VTR_train = VTR_train.vec.values\n",
    "X_VTR_train = repadding(X_VTR_train,20,100)\n",
    "print(X_VTR_train.shape)\n",
    "y_VTR_train = VTR_train.target.values\n",
    "print(y_VTR_train.shape)\n",
    "\n",
    "X_VTR_val = VTR_val.vec.values\n",
    "X_VTR_val = repadding(X_VTR_val,20,100)\n",
    "print(X_VTR_val.shape)\n",
    "y_VTR_val = VTR_val.target.values\n",
    "print(y_VTR_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGF_val = VGF_train.sample(frac=0.1)\n",
    "VGF_train = VGF_train.drop(labels=VGF_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4441, 20, 100)\n",
      "(4441,)\n",
      "(493, 20, 100)\n",
      "(493,)\n"
     ]
    }
   ],
   "source": [
    "X_VGF_train = VGF_train.vec.values\n",
    "X_VGF_train = unpack(X_VGF_train,20,100)\n",
    "print(X_VGF_train.shape)\n",
    "y_VGF_train = VGF_train.target.values\n",
    "print(y_VGF_train.shape)\n",
    "\n",
    "X_VGF_val = VGF_val.vec.values\n",
    "X_VGF_val = unpack(X_VGF_val,20,100)\n",
    "print(X_VGF_val.shape)\n",
    "y_VGF_val = VGF_val.target.values\n",
    "print(y_VGF_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_VTR_train.npy', X_VTR_train)\n",
    "np.save('y_VTR_train.npy', y_VTR_train)\n",
    "np.save('X_VTR_val.npy', X_VTR_val)\n",
    "np.save('y_VTR_val.npy', y_VTR_val)\n",
    "\n",
    "np.save('X_VGF_train.npy', X_VGF_train)\n",
    "np.save('y_VGF_train.npy', y_VGF_train)\n",
    "np.save('X_VGF_val.npy', X_VGF_val)\n",
    "np.save('y_VGF_val.npy', y_VGF_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_VTR_train = np.load('X_VTR_train.npy', allow_pickle=True)\n",
    "y_VTR_train = np.load('y_VTR_train.npy', allow_pickle=True)\n",
    "X_VTR_val = np.load('X_VTR_val.npy', allow_pickle=True)\n",
    "y_VTR_val = np.load('y_VTR_val.npy', allow_pickle=True)\n",
    "\n",
    "X_VGF_train = np.load('X_VGF_train.npy', allow_pickle=True)\n",
    "y_VGF_train = np.load('y_VGF_train.npy', allow_pickle=True)\n",
    "X_VGF_val = np.load('X_VGF_val.npy', allow_pickle=True)\n",
    "y_VGF_val = np.load('y_VGF_val.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4441, 20, 100) (4441,)\n",
      "(4441, 20, 100) (4441,)\n",
      "(493, 20, 100) (493,)\n",
      "(493, 20, 100) (493,)\n"
     ]
    }
   ],
   "source": [
    "print(X_VTR_train.shape, y_VTR_train.shape)\n",
    "print(X_VGF_train.shape, y_VGF_train.shape)\n",
    "\n",
    "print(X_VTR_val.shape, y_VTR_val.shape)\n",
    "print(X_VGF_val.shape, y_VGF_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4441 into shape (4441,20,100,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JORGEH~1\\AppData\\Local\\Temp/ipykernel_9912/3769361260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0my_VGF_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mX_VTR_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_VTR_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_VTR_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_VTR_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_VTR_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mX_VTR_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_VTR_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 4441 into shape (4441,20,100,1)"
     ]
    }
   ],
   "source": [
    "\n",
    "X_VTR_train = X_VTR_train.reshape(X_VTR_train.shape[0], X_VTR_train[0].shape[0], X_VTR_train[0].shape[1], 1)\n",
    "X_VTR_train = X_VTR_train.astype('float32')\n",
    "\n",
    "X_VGF_train = X_VGF_train.reshape(X_VGF_train.shape[0], X_VGF_train[0].shape[0], X_VGF_train[0].shape[1], 1)\n",
    "X_VGF_train = X_VGF_train.astype('float32')\n",
    "\n",
    "X_VTR_val = X_VTR_val.reshape(X_VTR_val.shape[0], X_VTR_val[0].shape[0], X_VTR_val[0].shape[1], 1)\n",
    "X_VTR_val = X_VTR_val.astype('float32')\n",
    "\n",
    "X_VGF_val = X_VGF_val.reshape(X_VGF_val.shape[0], X_VGF_val[0].shape[0], X_VGF_val[0].shape[1], 1)\n",
    "X_VGF_val = X_VGF_val.astype('float32')\n",
    "\n",
    "print(X_VTR_train.shape)\n",
    "print(y_VTR_train.shape)\n",
    "\n",
    "print(X_VGF_train.shape)\n",
    "print(y_VGF_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación usando CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dpF3dWuYFR2O"
   },
   "outputs": [],
   "source": [
    "# \"images\" sizes\n",
    "n1 = X_train.shape[1]\n",
    "n2 = X_train.shape[2]\n",
    "\n",
    "print(\"Las imágenes son de \",n1,\"x\",n2)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
    "# X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 20\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_val = np_utils.to_categorical(y_val, n_classes)\n",
    "# Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# convolutional layer\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(n1,n2,1)))\n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "# hidden layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model \n",
    "\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)\n",
    "history = model.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_val, Y_val)\n",
    "            ,callbacks=[early_callback])\n",
    "\n",
    "# score = model.evaluate(X_test, Y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "API_TextRank_GFractal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
